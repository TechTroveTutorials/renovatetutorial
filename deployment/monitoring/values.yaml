default:
  # List of environment variables applied to all components
  env:
    - name: OTEL_SERVICE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: "metadata.labels['app.kubernetes.io/component']"
    - name: OTEL_COLLECTOR_NAME
      value: '{{ include "monitoring.name" . }}-otelcol'
    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      value: cumulative
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo
  # Allows overriding and additions to .Values.default.env
  envOverrides: []
  #  - name: OTEL_K8S_NODE_NAME
  #    value: "someConstantValue"
  image:
    repository: ghcr.io/open-telemetry/demo
    # Overrides the image tag whose default is the chart appVersion.
    # The service's name will be applied to the end of this value.
    tag: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
  # Default # of replicas for all components
  replicas: 1
  # Default schedulingRules for all components
  schedulingRules:
    nodeSelector: {}
    affinity: {}
    tolerations: []
  # Default securityContext for all components
  securityContext: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

components:
  ## Demo Components are named objects (services) with several properties
  # demoService:
  ## Enable the component (service)
  #   enabled: true
  #   useDefault:
  ## Use default environment variables
  #     env: true
  ## Override Image repository and Tag. Tag will use appVersion as default.
  ## Component's name will be applied to end of this value.
  #   imageOverride: {}
  ## Optional service definitions to apply
  #   service:
  ## Service Type to use for this component. Default is ClusterIP.
  #     type: ClusterIP
  ## Service Port to use to expose this component. Default is nil
  #     port: 8080
  ## Service Node Port to use to expose this component on a NodePort service. Default is nil
  #     nodePort: 30080
  ## Service Annotations to add to this component
  #     annotations: {}
  ## Additional service ports to use to expose this component
  #   ports:
  #     - name: extraServicePort
  #       value: 8081
  ## Environment variables to add to the component's pod
  #   env:
  ## Environment variables that upsert (append + merge) into the `env` specification for this component.
  #   envOverrides:
  ## Pod Scheduling rules for nodeSelector, affinity, or tolerations.
  #   schedulingRules:
  #     nodeSelector: {}
  #     affinity: {}
  #     tolerations: []
  ## Pod Annotations to add to this component
  #   podAnnotations: {}
  ## Resources for this component
  #   resources: {}
  ## Container security context for setting user ID (UID), group ID (GID) and other security policies
  #   securityContext:
  ## Ingresses rules to add for the to the component
  # ingress:
  ## Enable the creation of Ingress rules. Default is false
  #   enabled: false
  ## Annotations to add to the ingress rule
  #   annotations: {}
  ## Which Ingress class (controller) to use. Default is unspecified.
  #   ingressClassName: nginx
  ## Hosts definitions for the Ingress rule
  #   hosts:
  #     - host: demo.example.com
  ## Each host can have multiple paths/routes
  #       paths:
  #         - path: /
  #           pathType: Prefix
  #           port: 8080
  ## Optional TLS specifications for the Ingress rule
  #   tls:
  #     - secretName: demo-tls
  #       hosts:
  #         - demo.example.com
  ## Additional ingresses - only created if ingress.enabled is true
  ## Useful for when differently annotated ingress services are required
  ## Each additional ingress needs key "name" set to something unique
  #   additionalIngresses: []
  #     - name: extra-demo-ingress
  #       ingressClassName: nginx
  #       annotations: {}
  #       hosts:
  #         - host: demo.example.com
  #           paths:
  #             - path: /
  #               pathType: Prefix
  #               port: 8080
  #       tls:
  #         - secretName: demo-tls
  #           hosts:
  #             - demo.example.com
  # # Command to use in the container spec, in case you don't want to go with the default command from the image.
  #   command: []
  # # Configuration to for this service; will create a ConfigMap, Volume, and Mount it into the container being spun up/.
  #   configuration: {}
  # # Kubernetes container health check options
  #   livenessProbe: {}
  # # Optional init container to run before the pod starts.
  #   initContainers:
  #     - name: <init-container-name>
  #       image: <init-container-image>
  #       command: [list of commands for the init container to run]
  # # Replicas for the component
  #  replicas: 1
  
opentelemetry-collector:
  enabled: true
  nameOverride: otelcol
  mode: deployment
  presets:
    kubernetesAttributes:
      enabled: true
  resources:
    limits:
      memory: 200Mi
  service:
    type: ClusterIP
  ports:
    metrics:
      enabled: true
    prometheus:
      enabled: true
      containerPort: 9464
      servicePort: 9464
      protocol: TCP
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9464"
    opentelemetry_community_demo: "true"
  config:
    receivers:
      otlp:
        protocols:
          http:
            # Since this collector needs to receive data from the web, enable cors for all origins
            # `allowed_origins` can be refined for your deployment domain
            cors:
              allowed_origins:
                - "http://*"
                - "https://*"

    exporters:
      ## Create an exporter to Jaeger using the standard `otlp` export format
      otlp:
        endpoint: '{{ include "monitoring.name" . }}-jaeger-collector:4317'
        tls:
          insecure: true
      # Create an exporter to Prometheus (metrics)
      otlphttp/prometheus:
        endpoint: 'http://{{ include "monitoring.name" . }}-prometheus-server:9090/api/v1/otlp'
        tls:
          insecure: true

    processors:
      resource:
        attributes:
        - key: service.instance.id
          from_attribute: k8s.pod.uid
          action: insert
      filter/ottl:
        error_mode: ignore
        metrics:
          metric:
            # FIXME: remove when a Metrics View is implemented in the checkout and productcatalog components
            # or when this issue is resolved: https://github.com/open-telemetry/opentelemetry-go-contrib/issues/3071
            - 'name == "rpc.server.duration"'
      transform:
        metric_statements:
          - context: metric
            statements:
              # FIXME: remove when this issue is resolved: https://github.com/open-telemetry/opentelemetry-java/issues/4834
              - set(description, "") where name == "queueSize"
              # FIXME: remove when these 2 issues are resolved:
              # Java: https://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/9478
              # Go: https://github.com/open-telemetry/opentelemetry-go-contrib/issues/4301
              - set(description, "") where name == "rpc.server.duration"
              # FIXME: remove when this issue is resolved: https://github.com/open-telemetry/opentelemetry-python-contrib/issues/1958
              - set(description, "") where name == "http.client.duration"

    connectors:
      spanmetrics:

    service:
      pipelines:
        traces:
          processors: [memory_limiter, resource, batch]
          exporters: [otlp, debug, spanmetrics]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [memory_limiter, filter/ottl, transform, resource, batch]
          exporters: [otlphttp/prometheus, debug]

jaeger:
  enabled: true
  provisionDataStore:
    cassandra: false
  allInOne:
    enabled: true
    args:
      - "--memory.max-traces=8000"
      - "--query.base-path=/jaeger/ui"
      - "--prometheus.server-url=http://{{ include \"monitoring.name\" . }}-prometheus-server:9090"
      - "--prometheus.query.normalize-calls=true"
      - "--prometheus.query.normalize-duration=true"
    extraEnv:
      - name: METRICS_STORAGE_TYPE
        value: prometheus
    resources:
      limits:
        memory: 300Mi
  storage:
    type: none
  agent:
    enabled: false
  collector:
    enabled: false
  query:
    enabled: false

prometheus:
  enabled: true
  alertmanager:
    enabled: false
  configmapReload:
    prometheus:
      enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false

  server:
    extraFlags:
      - "enable-feature=exemplar-storage"
      - "enable-feature=otlp-write-receiver"
    persistentVolume:
      enabled: false
    service:
      servicePort: 9090
    resources:
      limits:
        memory: 300Mi

  serverFiles:
    prometheus.yml:
      scrape_configs: []

grafana:
  enabled: true
  grafana.ini:
    auth:
      disable_login_form: true
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Admin
    server:
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana"
      serve_from_sub_path: true
  adminPassword: admin
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          uid: webstore-metrics
          type: prometheus
          url: 'http://{{ include "monitoring.name" . }}-prometheus-server:9090'
          editable: true
          isDefault: true
          jsonData:
            exemplarTraceIdDestinations:
              - datasourceUid: webstore-traces
                name: trace_id

              - url: http://localhost:8080/jaeger/ui/trace/$${__value.raw}
                name: trace_id
                urlDisplayLabel: View in Jaeger UI

        - name: Jaeger
          uid: webstore-traces
          type: jaeger
          url: 'http://{{ include "monitoring.name" . }}-jaeger-query:16686/jaeger/ui'
          editable: true
          isDefault: false
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboardsConfigMaps:
    default: '{{ include "monitoring.name" . }}-grafana-dashboards'
  resources:
    limits:
      memory: 150Mi